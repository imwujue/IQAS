{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## * 数据准备\n",
    "\n",
    "## * 特征工程\n",
    "#### - 提取字特征\n",
    "#### - 提取词特征\n",
    "#### - 提取图特征\n",
    "## * 模型建立"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据准备"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 将txt文件转换为csv文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import gc\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = open('./datasets/char_embed.txt')\n",
    "data1 = reader.readlines()\n",
    "with open(\"./datasets/char_embed.csv\",\"w\")as csvfile:\n",
    "    for row in data1:\n",
    "        row = row.split()\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(row)\n",
    "        \n",
    "reader = open('./datasets/word_embed.txt')\n",
    "data2 = reader.readlines()\n",
    "with open(\"./datasets/word_embed.csv\",\"w\")as csvfile:\n",
    "    for row in data2:\n",
    "        row = row.split()\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 读取csv文件，查看内容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_master = pd.read_csv('./datasets/train.csv')\n",
    "test_master = pd.read_csv('./datasets/test.csv')\n",
    "question = pd.read_csv('./datasets/question.csv')\n",
    "char_embed = pd.read_csv('./datasets/char_embed.csv',header=None)\n",
    "word_embed = pd.read_csv('./datasets/word_embed.csv',header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>q1</th>\n",
       "      <th>q2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Q397345</td>\n",
       "      <td>Q538594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Q193805</td>\n",
       "      <td>Q699273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>Q085471</td>\n",
       "      <td>Q676160</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label       q1       q2\n",
       "0      1  Q397345  Q538594\n",
       "1      0  Q193805  Q699273\n",
       "2      0  Q085471  Q676160"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_master.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>q1</th>\n",
       "      <th>q2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Q017571</td>\n",
       "      <td>Q006012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Q728241</td>\n",
       "      <td>Q542572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Q166997</td>\n",
       "      <td>Q118270</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        q1       q2\n",
       "0  Q017571  Q006012\n",
       "1  Q728241  Q542572\n",
       "2  Q166997  Q118270"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_master.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>words</th>\n",
       "      <th>chars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Q000000</td>\n",
       "      <td>W05733 W05284 W09158 W14968 W07863</td>\n",
       "      <td>L1128 L1861 L2218 L1796 L1055 L0847 L2927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Q000001</td>\n",
       "      <td>W17378 W17534 W03249 W01490 W18802</td>\n",
       "      <td>L2214 L1980 L0156 L1554 L2218 L1861 L3019 L010...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Q000002</td>\n",
       "      <td>W17378 W08158 W20171 W11246 W14759</td>\n",
       "      <td>L2214 L2350 L2568 L1969 L2168 L0694 L3012 L256...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       qid                               words  \\\n",
       "0  Q000000  W05733 W05284 W09158 W14968 W07863   \n",
       "1  Q000001  W17378 W17534 W03249 W01490 W18802   \n",
       "2  Q000002  W17378 W08158 W20171 W11246 W14759   \n",
       "\n",
       "                                               chars  \n",
       "0          L1128 L1861 L2218 L1796 L1055 L0847 L2927  \n",
       "1  L2214 L1980 L0156 L1554 L2218 L1861 L3019 L010...  \n",
       "2  L2214 L2350 L2568 L1969 L2168 L0694 L3012 L256...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "      <th>300</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>L0000</td>\n",
       "      <td>-0.546460</td>\n",
       "      <td>2.285091</td>\n",
       "      <td>-3.084309</td>\n",
       "      <td>1.064661</td>\n",
       "      <td>-2.090880</td>\n",
       "      <td>0.651496</td>\n",
       "      <td>-2.429877</td>\n",
       "      <td>-2.262385</td>\n",
       "      <td>-1.981884</td>\n",
       "      <td>...</td>\n",
       "      <td>0.207397</td>\n",
       "      <td>1.476373</td>\n",
       "      <td>0.863744</td>\n",
       "      <td>-0.341826</td>\n",
       "      <td>0.433234</td>\n",
       "      <td>-0.730324</td>\n",
       "      <td>0.215955</td>\n",
       "      <td>-0.528452</td>\n",
       "      <td>-0.340528</td>\n",
       "      <td>-2.018747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>L0001</td>\n",
       "      <td>-9.016356</td>\n",
       "      <td>-3.801084</td>\n",
       "      <td>-7.210567</td>\n",
       "      <td>3.052900</td>\n",
       "      <td>-1.340958</td>\n",
       "      <td>1.395385</td>\n",
       "      <td>-5.482922</td>\n",
       "      <td>-7.407759</td>\n",
       "      <td>-3.611857</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.818142</td>\n",
       "      <td>4.968217</td>\n",
       "      <td>-4.254042</td>\n",
       "      <td>-0.709047</td>\n",
       "      <td>1.288105</td>\n",
       "      <td>-1.222849</td>\n",
       "      <td>-5.521402</td>\n",
       "      <td>-2.653049</td>\n",
       "      <td>1.868731</td>\n",
       "      <td>2.147064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>L0002</td>\n",
       "      <td>-0.138824</td>\n",
       "      <td>0.219148</td>\n",
       "      <td>-0.890053</td>\n",
       "      <td>0.106301</td>\n",
       "      <td>-0.494364</td>\n",
       "      <td>0.550745</td>\n",
       "      <td>-0.279467</td>\n",
       "      <td>-0.048898</td>\n",
       "      <td>-0.021813</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247383</td>\n",
       "      <td>0.576698</td>\n",
       "      <td>1.261507</td>\n",
       "      <td>0.446992</td>\n",
       "      <td>-0.418965</td>\n",
       "      <td>-0.278471</td>\n",
       "      <td>1.426156</td>\n",
       "      <td>-0.579678</td>\n",
       "      <td>-0.322354</td>\n",
       "      <td>-0.661302</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 301 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0         1         2         3         4         5         6    \\\n",
       "0  L0000 -0.546460  2.285091 -3.084309  1.064661 -2.090880  0.651496   \n",
       "1  L0001 -9.016356 -3.801084 -7.210567  3.052900 -1.340958  1.395385   \n",
       "2  L0002 -0.138824  0.219148 -0.890053  0.106301 -0.494364  0.550745   \n",
       "\n",
       "        7         8         9      ...          291       292       293  \\\n",
       "0 -2.429877 -2.262385 -1.981884    ...     0.207397  1.476373  0.863744   \n",
       "1 -5.482922 -7.407759 -3.611857    ...    -0.818142  4.968217 -4.254042   \n",
       "2 -0.279467 -0.048898 -0.021813    ...     0.247383  0.576698  1.261507   \n",
       "\n",
       "        294       295       296       297       298       299       300  \n",
       "0 -0.341826  0.433234 -0.730324  0.215955 -0.528452 -0.340528 -2.018747  \n",
       "1 -0.709047  1.288105 -1.222849 -5.521402 -2.653049  1.868731  2.147064  \n",
       "2  0.446992 -0.418965 -0.278471  1.426156 -0.579678 -0.322354 -0.661302  \n",
       "\n",
       "[3 rows x 301 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_embed.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "      <th>300</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>W00000</td>\n",
       "      <td>0.169316</td>\n",
       "      <td>-0.063898</td>\n",
       "      <td>0.115286</td>\n",
       "      <td>-0.077671</td>\n",
       "      <td>0.067184</td>\n",
       "      <td>0.019339</td>\n",
       "      <td>0.039596</td>\n",
       "      <td>-0.026229</td>\n",
       "      <td>-0.160078</td>\n",
       "      <td>...</td>\n",
       "      <td>0.061151</td>\n",
       "      <td>0.044519</td>\n",
       "      <td>-0.194827</td>\n",
       "      <td>0.122456</td>\n",
       "      <td>0.122785</td>\n",
       "      <td>-0.154153</td>\n",
       "      <td>-0.116578</td>\n",
       "      <td>-0.127786</td>\n",
       "      <td>0.110593</td>\n",
       "      <td>-0.171084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>W00001</td>\n",
       "      <td>1.548212</td>\n",
       "      <td>-1.052776</td>\n",
       "      <td>1.192632</td>\n",
       "      <td>0.760363</td>\n",
       "      <td>1.594398</td>\n",
       "      <td>1.478917</td>\n",
       "      <td>-1.555349</td>\n",
       "      <td>0.401968</td>\n",
       "      <td>1.588316</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.898932</td>\n",
       "      <td>0.129864</td>\n",
       "      <td>-2.062325</td>\n",
       "      <td>0.068316</td>\n",
       "      <td>0.540282</td>\n",
       "      <td>-1.682620</td>\n",
       "      <td>-0.816290</td>\n",
       "      <td>-1.464458</td>\n",
       "      <td>-0.361792</td>\n",
       "      <td>0.943322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>W00002</td>\n",
       "      <td>0.934084</td>\n",
       "      <td>0.106135</td>\n",
       "      <td>-0.391749</td>\n",
       "      <td>-0.209661</td>\n",
       "      <td>-0.558696</td>\n",
       "      <td>-0.942362</td>\n",
       "      <td>-0.274353</td>\n",
       "      <td>-0.232077</td>\n",
       "      <td>-1.024267</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.357264</td>\n",
       "      <td>-0.451105</td>\n",
       "      <td>-0.724659</td>\n",
       "      <td>0.525233</td>\n",
       "      <td>0.290343</td>\n",
       "      <td>0.357838</td>\n",
       "      <td>-0.042750</td>\n",
       "      <td>1.315442</td>\n",
       "      <td>-0.167775</td>\n",
       "      <td>-0.393665</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 301 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0         1         2         3         4         5         6    \\\n",
       "0  W00000  0.169316 -0.063898  0.115286 -0.077671  0.067184  0.019339   \n",
       "1  W00001  1.548212 -1.052776  1.192632  0.760363  1.594398  1.478917   \n",
       "2  W00002  0.934084  0.106135 -0.391749 -0.209661 -0.558696 -0.942362   \n",
       "\n",
       "        7         8         9      ...          291       292       293  \\\n",
       "0  0.039596 -0.026229 -0.160078    ...     0.061151  0.044519 -0.194827   \n",
       "1 -1.555349  0.401968  1.588316    ...    -1.898932  0.129864 -2.062325   \n",
       "2 -0.274353 -0.232077 -1.024267    ...    -0.357264 -0.451105 -0.724659   \n",
       "\n",
       "        294       295       296       297       298       299       300  \n",
       "0  0.122456  0.122785 -0.154153 -0.116578 -0.127786  0.110593 -0.171084  \n",
       "1  0.068316  0.540282 -1.682620 -0.816290 -1.464458 -0.361792  0.943322  \n",
       "2  0.525233  0.290343  0.357838 -0.042750  1.315442 -0.167775 -0.393665  \n",
       "\n",
       "[3 rows x 301 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_embed.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 特征工程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 提取train集合字特征"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.DataFrame(train_master)\n",
    "train = pd.merge(train, question, left_on = ['q1'], right_on = ['qid'], how = 'left')\n",
    "train = pd.merge(train, question, left_on = ['q2'], right_on = ['qid'], how = 'left')\n",
    "train = train[['label', 'words_x','words_y']]\n",
    "train.columns = ['label', 'q1', 'q2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embed.index = word_embed[0]\n",
    "word = word_embed.index.values\n",
    "word_to_index = dict([(word[i],i) for i in range(len(word))])\n",
    "index_to_word = dict([(i, word[i]) for i in range(len(word))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_q1 = train.q1.values\n",
    "train_q2 = train.q2.values\n",
    "train_shape_q1 = train_q1.shape[0]\n",
    "train_shape_q2 = train_q2.shape[0]\n",
    "max_len = 20\n",
    "embed_dim = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_q1_indices = np.zeros((train_shape_q1,max_len))\n",
    "for i in range(train_shape_q1):\n",
    "    sentence_words_q1 = train_q1[i].split(' ')\n",
    "    for j,w in enumerate(sentence_words_q1):\n",
    "        if j >= max_len:\n",
    "            break\n",
    "        train_q1_indices[i, j] = word_to_index[w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_q2_indices = np.zeros((train_shape_q2,max_len))\n",
    "for i in range(train_shape_q2):\n",
    "    sentence_words_q2 = train_q2[i].split(' ')\n",
    "    for j,w in enumerate(sentence_words_q2):\n",
    "        if j >= max_len:\n",
    "            break\n",
    "        train_q2_indices[i, j] = word_to_index[w]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 计算q1，q2长度差异"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.         0.         0.28571429 ... 0.14285714 0.3        0.5       ]\n"
     ]
    }
   ],
   "source": [
    "merge = train[['q1', 'q2']]\n",
    "q1_len = merge.q1.apply(lambda x: len(x.split(' '))).values\n",
    "q2_len = merge.q2.apply(lambda x: len(x.split(' '))).values\n",
    "len_diff = np.abs((q1_len - q2_len))/np.max([q1_len, q2_len], axis=0)\n",
    "# print(len_diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 计算q1，q2中相同的字的个数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1_word_set = merge.q1.apply(lambda x: x.split(' ')).apply(set).values\n",
    "q2_word_set = merge.q2.apply(lambda x: x.split(' ')).apply(set).values\n",
    "result = [len(q1_word_set[i] & q2_word_set[i]) for i in range(max(len(q1_word_set), len(q2_word_set)))]\n",
    "result = pd.DataFrame(result)\n",
    "result.columns = ['num_common_words']\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 计算共现字比例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio = [len(q1_word_set[i] & q2_word_set[i])/max(q1_len[i], q2_len[i]) for i in range(len(q1_word_set))]\n",
    "ratio = pd.DataFrame(ratio)\n",
    "result.columns = ['common_word_ratio']\n",
    "# print(ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 计算tf-idf字向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(merge.q2.values)\n",
    "sorted(merge.q2.values)\n",
    "vectorizer = TfidfVectorizer().fit(question.words.values)\n",
    "q1_tfidf = vectorizer.transform(merge.q1.values)\n",
    "q2_tfidf = vectorizer.transform(merge.q2.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 根据tf-idf系数调整共现字比例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjusted_common_word_ratio = []\n",
    "for i in range(0,q1_tfidf.shape[0]):\n",
    "    q1words = {}\n",
    "    q2words = {}\n",
    "    for word in merge.loc[i, 'q1'].split():\n",
    "        q1words[word] = q1words.get(word,0)+1\n",
    "#         print(q1words[word])\n",
    "    for word in merge.loc[i, 'q2'].split():\n",
    "        q2words[word] = q2words.get(word,0)+1\n",
    "#         print(q2words[word])\n",
    "    sum_shared_word_in_q1 = sum([q1words[w] * q1_tfidf[i,word_to_index[w]] for w in q1words if w in q2words])\n",
    "#     print(sum_shared_word_in_q1)\n",
    "    sum_shared_word_in_q2 = sum([q2words[w] * q2_tfidf[i,word_to_index[w]] for w in q2words if w in q1words])\n",
    "#     print(sum_shared_word_in_q2)\n",
    "    sum1 = sum(q1words[w] * q1_tfidf[i,word_to_index[w]] for w in q1words if word_to_index[w] != 20890) \n",
    "    sum2 = sum(q2words[w] * q2_tfidf[i,word_to_index[w]] for w in q2words)\n",
    "    sum_tol = sum1 + sum2\n",
    "#     print(sum_tol)\n",
    "    if sum_tol<1e-6:\n",
    "        adjusted_common_word_ratio.append(0.0)\n",
    "    else:\n",
    "#         print(sum_shared_word_in_q1)\n",
    "#         print(sum_shared_word_in_q2)\n",
    "#         print(sum_tol)\n",
    "#         print()\n",
    "        adjusted_common_word_ratio.append(1.0 * (sum_shared_word_in_q1 + sum_shared_word_in_q2)/sum_tol)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 计算字影响力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_power = {}\n",
    "for i in train.index:\n",
    "    label = int(train.loc[i, 'label'])\n",
    "    q1_words = train.loc[i, 'q1'].lower().split()\n",
    "    q2_words = train.loc[i, 'q2'].lower().split()\n",
    "    all_words = set(q1_words + q2_words)\n",
    "    q1_words = set(q1_words)\n",
    "    q2_words = set(q2_words)\n",
    "    for word in all_words:\n",
    "        if word not in words_power:\n",
    "            words_power[word] = [0. for i in range(7)]\n",
    "        words_power[word][0] += 1.            \n",
    "        words_power[word][1] += 1.\n",
    "        if ((word in q1_words) and (word not in q2_words)) or ((word not in q1_words) and (word in q2_words)):\n",
    "            words_power[word][3] += 1.\n",
    "            if label == 0:\n",
    "                words_power[word][2] += 1.\n",
    "                words_power[word][4] += 1.\n",
    "        if (word in q1_words) and (word in q2_words):\n",
    "            words_power[word][5] += 1.\n",
    "            if label == 1:\n",
    "                words_power[word][2] += 1.\n",
    "                words_power[word][6] += 1.\n",
    "for word in words_power:\n",
    "    words_power[word][1]  /= train.shape[0]\n",
    "    words_power[word][2] /= words_power[word][0]\n",
    "    if words_power[word][3] > 1e-6:\n",
    "        words_power[word][4] /= words_power[word][3]        \n",
    "    words_power[word][5] /= words_power[word][0]\n",
    "sorted_words_power = sorted(words_power.items(), key =lambda d: d[1][0], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 预测有双侧影响力的字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh_num, thresh_rate = 7, 0.3\n",
    "pword_dside = []\n",
    "pword = sorted_words_power\n",
    "pword = filter(lambda x: x[1][0] * x[1][5] >=thresh_num , pword)\n",
    "pword_sort = sorted(pword, key = lambda d: d[1][6], reverse = True)\n",
    "pword_dside.extend(map(lambda x: x[0], filter(lambda x: x[1][6]>=thresh_rate, pword_sort)))\n",
    "merge = train[['q1', 'q2']]\n",
    "pword_dside_tags = []\n",
    "for i in merge.index:\n",
    "    tags = []\n",
    "    q1_words = set(merge.loc[i, 'q1'].lower().split())\n",
    "    q2_words = set(merge.loc[i, 'q2'].lower().split())\n",
    "    for word in pword_dside:\n",
    "        if (word in q1_words) and (word in q2_words):\n",
    "            tags.append(1.0)\n",
    "        else:\n",
    "            tags.append(0.0)\n",
    "    pword_dside_tags.append(tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 预测有单侧影响力的字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh_num, thresh_rate = 7, 0.3\n",
    "pword_oside = []\n",
    "pword = sorted_words_power\n",
    "pword = filter(lambda x: x[1][0] * x[1][5] >=thresh_num , pword)\n",
    "pword_oside.extend(map(lambda x: x[0], filter(lambda x: x[1][4]>thresh_rate, pword)))\n",
    "merge = train[['q1', 'q2']]\n",
    "pword_oside_tags = []\n",
    "for i in merge.index:\n",
    "    tags = []\n",
    "    q1_words = set(merge.loc[i, 'q1'].lower().split())\n",
    "    q2_words = set(merge.loc[i, 'q2'].lower().split())\n",
    "    for word in pword_oside:\n",
    "        if (word in q1_words) and (word not in q2_words):\n",
    "            tags.append(1.0)\n",
    "        else:\n",
    "            tags.append(0.0)\n",
    "    pword_oside_tags.append(tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 计算单字的双侧影响力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_least = 300\n",
    "merge = train[['q1', 'q2']]\n",
    "words_power = dict(sorted_words_power)\n",
    "pword_dside_rate = []\n",
    "for i in merge.index:\n",
    "    rate = 1.0\n",
    "    q1_words = set(merge.loc[i, 'q1'].lower().split())\n",
    "    q2_words = set(merge.loc[i, 'q2'].lower().split())\n",
    "    share_words = list(q1_words.intersection(q2_words))\n",
    "    for word in share_words:\n",
    "        if word in pword_dside:\n",
    "            rate *= (1.0 - words_power[word][6])\n",
    "    pword_dside_rate.append(1-rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 计算单字的单侧影响力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_least = 300\n",
    "merge = train[['q1', 'q2']]\n",
    "words_power = dict(sorted_words_power)\n",
    "pword_oside_rate = []\n",
    "for i in merge.index:\n",
    "    rate = 1.0\n",
    "    q1_words = set(merge.loc[i, 'q1'].lower().split())\n",
    "    q2_words = set(merge.loc[i, 'q2'].lower().split())\n",
    "    q1_diff = list(set(q1_words).difference(set(q2_words)))\n",
    "    q2_diff = list(set(q2_words).difference(set(q1_words)))\n",
    "    all_diff = set(q1_diff + q2_diff)\n",
    "    for word in all_diff:\n",
    "        if word in pword_oside:\n",
    "            rate *= (1.0-words_power[word][4])\n",
    "    pword_oside_rate.append(1-rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 计算可编辑距离"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edit_distance(q1, q2):\n",
    "    str1 = q1.split(' ')\n",
    "    str2 = q2.split(' ')\n",
    "    matrix = [[i+j for j in range(len(str2)+1)] for i in range(len(str1)+1)]\n",
    "    for i in range(1, len(str1)+1):\n",
    "        for j in range(1, len(str2)+1):\n",
    "            if str1[i-1] == str2[j-1]:\n",
    "                d = 0\n",
    "            else:\n",
    "                d = 1\n",
    "            matrix[i][j] = min(matrix[i-1][j]+1, matrix[i][j-1]+1, matrix[i-1][j-1]+d)\n",
    "        if i>1 and j >1 and str1[i-1] == str2[j-2] and str1[i-2] == str2[j-1]:\n",
    "            d = 0\n",
    "            matrix[i][j] = min(matrix[i][j], matrix[i-2][j-2]+d)\n",
    "    return matrix[len(str1)][len(str2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1_len = merge['q1'].apply(lambda x: len(x.split(' '))).values\n",
    "q2_len = merge['q2'].apply(lambda x: len(x.split(' '))).values\n",
    "dist =[edit_distance(merge.loc[i,'q1'],merge.loc[i,'q2'])/np.max([q1_len,q2_len],axis=0)[i] for i in merge.index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 合并提取的字特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjusted_common_word_ratio = pd.DataFrame(adjusted_common_word_ratio)\n",
    "edit_distance = pd.DataFrame(dist)\n",
    "len_diff = pd.DataFrame(len_diff)\n",
    "pword_dside_rate = pd.DataFrame(pword_dside_rate)\n",
    "pword_oside_rate = pd.DataFrame(pword_oside_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = pd.merge(adjusted_common_word_ratio, edit_distance, left_index = True,right_index =  True)\n",
    "train_features = pd.merge(train_features, len_diff, left_index = True,right_index =  True)\n",
    "train_features = pd.merge(train_features, pword_dside_rate, left_index = True,right_index =  True)\n",
    "train_features = pd.merge(train_features, pword_oside_rate, left_index = True,right_index =  True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features.columns = ['adjusted_common_word_ratio','edit_distance','len_diff','pword_dside_rate','pword_oside_rate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features.to_csv('output/train_feature_words.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del train, merge, adjusted_common_word_ratio, edit_distance, len_diff, pword_dside_rate, pword_oside_rate\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 提取词特征"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.DataFrame(train_master)\n",
    "train = pd.merge(train, question, left_on = ['q1'], right_on = ['qid'], how = 'left')\n",
    "train = pd.merge(train, question, left_on = ['q2'], right_on = ['qid'], how = 'left')\n",
    "train = train[['label', 'chars_x','chars_y']]\n",
    "train.columns = ['label', 'q1', 'q2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_embed.index = char_embed[0]\n",
    "char = char_embed.index.values\n",
    "char_to_index = dict([(char[i],i) for i in range(len(char))])\n",
    "index_to_char = dict([(i, char[i]) for i in range(len(char))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_q1 = train.q1.values\n",
    "train_q2 = train.q2.values\n",
    "train_shape_q1 = train_q1.shape[0]\n",
    "train_shape_q2 = train_q2.shape[0]\n",
    "max_len = 20\n",
    "embed_dim = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_q1_indices = np.zeros((train_shape_q1,max_len))\n",
    "for i in range(train_shape_q1):\n",
    "    sentence_chars_q1 = train_q1[i].split(' ')\n",
    "    for j,w in enumerate(sentence_chars_q1):\n",
    "        if j >= max_len:\n",
    "            break\n",
    "        train_q1_indices[i, j] = char_to_index[w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_q2_indices = np.zeros((train_shape_q1,max_len))\n",
    "for i in range(train_shape_q2):\n",
    "    sentence_chars_q1 = train_q2[i].split(' ')\n",
    "    for j,w in enumerate(sentence_chars_q1):\n",
    "        if j >= max_len:\n",
    "            break\n",
    "        train_q2_indices[i, j] = char_to_index[w]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 计算q1，q2长度差异"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge = train[['q1', 'q2']]\n",
    "q1_len = merge.q1.apply(lambda x: len(x.split(' '))).values\n",
    "q2_len = merge.q2.apply(lambda x: len(x.split(' '))).values\n",
    "len_diff = np.abs((q1_len - q2_len))/np.max([q1_len, q2_len], axis=0)\n",
    "# print(len_diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 计算q1，q2中相同的词的个数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1_word_set = merge.q1.apply(lambda x: x.split(' ')).apply(set).values\n",
    "q2_word_set = merge.q2.apply(lambda x: x.split(' ')).apply(set).values\n",
    "result = [len(q1_word_set[i] & q2_word_set[i]) for i in range(max(len(q1_word_set), len(q2_word_set)))]\n",
    "result = pd.DataFrame(result)\n",
    "result.columns = ['num_common_words']\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 计算共现词比例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio = [len(q1_word_set[i] & q2_word_set[i])/max(q1_len[i], q2_len[i]) for i in range(len(q1_word_set))]\n",
    "ratio = pd.DataFrame(ratio)\n",
    "result.columns = ['common_word_ratio']\n",
    "# print(ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 计算tf-idf词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(merge.q2.values)\n",
    "sorted(merge.q2.values)\n",
    "vectorizer = TfidfVectorizer().fit(question.words.values)\n",
    "q1_tfidf = vectorizer.transform(merge.q1.values)\n",
    "q2_tfidf = vectorizer.transform(merge.q2.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 根据tf-idf系数调整共现词比例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjusted_common_word_ratio = []\n",
    "for i in range(0,q1_tfidf.shape[0]):\n",
    "    q1words = {}\n",
    "    q2words = {}\n",
    "    for word in merge.loc[i, 'q1'].split():\n",
    "        q1words[word] = q1words.get(word,0)+1\n",
    "    for word in merge.loc[i, 'q2'].split():\n",
    "        q2words[word] = q2words.get(word,0)+1\n",
    "    sum_shared_word_in_q1 = sum([q1words[w] * q1_tfidf[i, char_to_index[w]] for w in q1words if w in q2words])\n",
    "    sum_shared_word_in_q2 = sum([q2words[w] * q2_tfidf[i,char_to_index[w]] for w in q2words if w in q1words])\n",
    "    sum1 = sum(q1words[w] * q1_tfidf[i,char_to_index[w]] for w in q1words) \n",
    "    sum2 = sum(q2words[w] * q2_tfidf[i,char_to_index[w]] for w in q2words)\n",
    "    sum_tol = sum1 + sum2\n",
    "    if sum_tol<1e-6:\n",
    "        adjusted_common_word_ratio.append(0.0)\n",
    "    else:\n",
    "        adjusted_common_word_ratio.append(1.0 * (sum_shared_word_in_q1 + sum_shared_word_in_q2)/sum_tol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 计算词影响力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_power = {}\n",
    "for i in train.index:\n",
    "    label = int(train.loc[i, 'label'])\n",
    "    q1_words = train.loc[i, 'q1'].lower().split()\n",
    "    q2_words = train.loc[i, 'q2'].lower().split()\n",
    "    all_words = set(q1_words + q2_words)\n",
    "    q1_words = set(q1_words)\n",
    "    q2_words = set(q2_words)\n",
    "    for word in all_words:\n",
    "        if word not in words_power:\n",
    "            words_power[word] = [0. for i in range(7)]\n",
    "        words_power[word][0] += 1.            \n",
    "        words_power[word][1] += 1.\n",
    "        if ((word in q1_words) and (word not in q2_words)) or ((word not in q1_words) and (word in q2_words)):\n",
    "            words_power[word][3] += 1.\n",
    "            if label == 0:\n",
    "                words_power[word][2] += 1.\n",
    "                words_power[word][4] += 1.\n",
    "        if (word in q1_words) and (word in q2_words):\n",
    "            words_power[word][5] += 1.\n",
    "            if label == 1:\n",
    "                words_power[word][2] += 1.\n",
    "                words_power[word][6] += 1.\n",
    "for word in words_power:\n",
    "    words_power[word][1]  /= train.shape[0]\n",
    "    words_power[word][2] /= words_power[word][0]\n",
    "    if words_power[word][3] > 1e-6:\n",
    "        words_power[word][4] /= words_power[word][3]        \n",
    "    words_power[word][5] /= words_power[word][0]\n",
    "sorted_words_power = sorted(words_power.items(), key =lambda d: d[1][0], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 预测有双侧影响力的词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh_num, thresh_rate = 7, 0.3\n",
    "pword_dside = []\n",
    "pword = sorted_words_power\n",
    "pword = filter(lambda x: x[1][0] * x[1][5] >=thresh_num , pword)\n",
    "pword_sort = sorted(pword, key = lambda d: d[1][6], reverse = True)\n",
    "pword_dside.extend(map(lambda x: x[0], filter(lambda x: x[1][6]>=thresh_rate, pword_sort)))\n",
    "merge = train[['q1', 'q2']]\n",
    "pword_dside_tags = []\n",
    "for i in merge.index:\n",
    "    tags = []\n",
    "    q1_words = set(merge.loc[i, 'q1'].lower().split())\n",
    "    q2_words = set(merge.loc[i, 'q2'].lower().split())\n",
    "    for word in pword_dside:\n",
    "        if (word in q1_words) and (word in q2_words):\n",
    "            tags.append(1.0)\n",
    "        else:\n",
    "            tags.append(0.0)\n",
    "    pword_dside_tags.append(tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 预测有单侧影响力的词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh_num, thresh_rate = 7, 0.3\n",
    "pword_oside = []\n",
    "pword = sorted_words_power\n",
    "pword = filter(lambda x: x[1][0] * x[1][5] >=thresh_num , pword)\n",
    "pword_oside.extend(map(lambda x: x[0], filter(lambda x: x[1][4]>thresh_rate, pword)))\n",
    "merge = train[['q1', 'q2']]\n",
    "pword_oside_tags = []\n",
    "for i in merge.index:\n",
    "    tags = []\n",
    "    q1_words = set(merge.loc[i, 'q1'].lower().split())\n",
    "    q2_words = set(merge.loc[i, 'q2'].lower().split())\n",
    "    for word in pword_oside:\n",
    "        if (word in q1_words) and (word not in q2_words):\n",
    "            tags.append(1.0)\n",
    "        else:\n",
    "            tags.append(0.0)\n",
    "    pword_oside_tags.append(tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 计算词的双侧影响力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_least = 300\n",
    "merge = train[['q1', 'q2']]\n",
    "words_power = dict(sorted_words_power)\n",
    "pword_dside_rate = []\n",
    "for i in merge.index:\n",
    "    rate = 1.0\n",
    "    q1_words = set(merge.loc[i, 'q1'].lower().split())\n",
    "    q2_words = set(merge.loc[i, 'q2'].lower().split())\n",
    "    share_words = list(q1_words.intersection(q2_words))\n",
    "    for word in share_words:\n",
    "        if word in pword_dside:\n",
    "            rate *= (1.0 - words_power[word][6])\n",
    "    pword_dside_rate.append(1-rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 预测词的单侧影响力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_least = 300\n",
    "merge = train[['q1', 'q2']]\n",
    "words_power = dict(sorted_words_power)\n",
    "pword_oside_rate = []\n",
    "for i in merge.index:\n",
    "    rate = 1.0\n",
    "    q1_words = set(merge.loc[i, 'q1'].lower().split())\n",
    "    q2_words = set(merge.loc[i, 'q2'].lower().split())\n",
    "    q1_diff = list(set(q1_words).difference(set(q2_words)))\n",
    "    q2_diff = list(set(q2_words).difference(set(q1_words)))\n",
    "    all_diff = set(q1_diff + q2_diff)\n",
    "    for word in all_diff:\n",
    "        if word in pword_oside:\n",
    "            rate *= (1.0-words_power[word][4])\n",
    "    pword_oside_rate.append(1-rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 合并提取的词特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjusted_common_char_ratio = pd.DataFrame(adjusted_common_word_ratio)\n",
    "pchar_dside_rate = pd.DataFrame(pword_dside_rate)\n",
    "pchar_oside_rate = pd.DataFrame(pword_oside_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = pd.read_csv('output/train_feature_words.csv', index_col=0)\n",
    "train_features = pd.DataFrame(train_features)\n",
    "train_features = pd.merge(train_features, adjusted_common_char_ratio, left_index = True,right_index =  True)\n",
    "train_features = pd.merge(train_features, pchar_dside_rate, left_index = True,right_index =  True)\n",
    "train_features = pd.merge(train_features, pchar_oside_rate, left_index = True,right_index =  True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features.columns = ['adjusted_common_word_ratio','edit_distance','len_diff',\n",
    "                          'pword_dside_rate','pword_oside_rate','adjusted_common_char_ratio','pchar_dside_rate','pchar_oside_rate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features.to_csv('output/train_feature.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 提取test集合字特征"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.DataFrame(test_master)\n",
    "test = pd.merge(test, question, left_on = ['q1'], right_on = ['qid'], how = 'left')\n",
    "test = pd.merge(test, question, left_on = ['q2'], right_on = ['qid'], how = 'left')\n",
    "test = test[['words_x','words_y']]\n",
    "test.columns = ['q1', 'q2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_q1 = test.q1.values\n",
    "test_q2 = test.q2.values\n",
    "test_shape_q1 = test_q1.shape[0]\n",
    "test_shape_q2 = test_q2.shape[0]\n",
    "max_len = 20\n",
    "embed_dim = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embed.index = word_embed[0]\n",
    "word = word_embed.index.values\n",
    "word_to_index = dict([(word[i],i) for i in range(len(word))])\n",
    "index_to_word = dict([(i, word[i]) for i in range(len(word))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_q1_indices = np.zeros((test_shape_q1,max_len))\n",
    "for i in range(test_shape_q1):\n",
    "    sentence_words_q1 = test_q1[i].split(' ')\n",
    "    for j,w in enumerate(sentence_words_q1):\n",
    "        if j >= max_len:\n",
    "            break\n",
    "        test_q1_indices[i, j] = word_to_index[w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_q2_indices = np.zeros((test_shape_q2,max_len))\n",
    "for i in range(test_shape_q2):\n",
    "    sentence_words_q2 = test_q2[i].split(' ')\n",
    "    for j,w in enumerate(sentence_words_q2):\n",
    "        if j >= max_len:\n",
    "            break\n",
    "        test_q2_indices[i, j] = word_to_index[w]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 计算q1，q2长度差异"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge = test[['q1', 'q2']]\n",
    "q1_len = merge.q1.apply(lambda x: len(x.split(' '))).values\n",
    "q2_len = merge.q2.apply(lambda x: len(x.split(' '))).values\n",
    "len_diff = np.abs((q1_len - q2_len))/np.max([q1_len, q2_len], axis=0)\n",
    "# print(len_diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 计算q1，q2中相同的字的个数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1_word_set = merge.q1.apply(lambda x: x.split(' ')).apply(set).values\n",
    "q2_word_set = merge.q2.apply(lambda x: x.split(' ')).apply(set).values\n",
    "result = [len(q1_word_set[i] & q2_word_set[i]) for i in range(max(len(q1_word_set), len(q2_word_set)))]\n",
    "result = pd.DataFrame(result)\n",
    "result.columns = ['num_common_words']\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 计算共现字比例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio = [len(q1_word_set[i] & q2_word_set[i])/max(q1_len[i], q2_len[i]) for i in range(len(q1_word_set))]\n",
    "ratio = pd.DataFrame(ratio)\n",
    "result.columns = ['common_word_ratio']\n",
    "# print(ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 计算tfi-df字向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(merge.q2.values)\n",
    "sorted(merge.q2.values)\n",
    "vectorizer = TfidfVectorizer().fit(question.words.values)\n",
    "q1_tfidf = vectorizer.transform(merge.q1.values)\n",
    "q2_tfidf = vectorizer.transform(merge.q2.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 根据tf-idf系数调整共现词比例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjusted_common_word_ratio = []\n",
    "count =0\n",
    "for i in range(0,q1_tfidf.shape[0]):\n",
    "    q1words = {}\n",
    "    q2words = {}\n",
    "    for word in merge.loc[i, 'q1'].split():\n",
    "        q1words[word] = q1words.get(word,0)+1\n",
    "    for word in merge.loc[i, 'q2'].split():\n",
    "        q2words[word] = q2words.get(word,0)+1\n",
    "    sum_shared_word_in_q1 = sum([q1words[w] * q1_tfidf[i,word_to_index[w]] for w in q1words if w in q2words])\n",
    "    sum_shared_word_in_q2 = sum([q2words[w] * q2_tfidf[i,word_to_index[w]] for w in q2words if w in q1words])\n",
    "    sum1 = sum(q1words[w] * q1_tfidf[i,word_to_index[w]] for w in q1words) \n",
    "    sum2 = sum(q2words[w] * q2_tfidf[i,word_to_index[w]] for w in q2words)\n",
    "    sum_tol = sum1 + sum2\n",
    "    if sum_tol<1e-6:\n",
    "        adjusted_common_word_ratio.append(0.0)\n",
    "    else:\n",
    "        adjusted_common_word_ratio.append(1.0 * (sum_shared_word_in_q1 + sum_shared_word_in_q2)/sum_tol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 计算字影响力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_power = {}\n",
    "for i in train.index:\n",
    "    label = int(train.loc[i, 'label'])\n",
    "    q1_words = train.loc[i, 'q1'].lower().split()\n",
    "    q2_words = train.loc[i, 'q2'].lower().split()\n",
    "    all_words = set(q1_words + q2_words)\n",
    "    q1_words = set(q1_words)\n",
    "    q2_words = set(q2_words)\n",
    "    for word in all_words:\n",
    "        if word not in words_power:\n",
    "            words_power[word] = [0. for i in range(7)]\n",
    "        words_power[word][0] += 1.            \n",
    "        words_power[word][1] += 1.\n",
    "        if ((word in q1_words) and (word not in q2_words)) or ((word not in q1_words) and (word in q2_words)):\n",
    "            words_power[word][3] += 1.\n",
    "            if label == 0:\n",
    "                words_power[word][2] += 1.\n",
    "                words_power[word][4] += 1.\n",
    "        if (word in q1_words) and (word in q2_words):\n",
    "            words_power[word][5] += 1.\n",
    "            if label == 1:\n",
    "                words_power[word][2] += 1.\n",
    "                words_power[word][6] += 1.\n",
    "for word in words_power:\n",
    "    words_power[word][1]  /= train.shape[0]\n",
    "    words_power[word][2] /= words_power[word][0]\n",
    "    if words_power[word][3] > 1e-6:\n",
    "        words_power[word][4] /= words_power[word][3]        \n",
    "    words_power[word][5] /= words_power[word][0]\n",
    "sorted_words_power = sorted(words_power.items(), key =lambda d: d[1][0], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 预测具有双侧影响力的字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh_num, thresh_rate = 7, 0.3\n",
    "pword_dside = []\n",
    "pword = sorted_words_power\n",
    "pword = filter(lambda x: x[1][0] * x[1][5] >=thresh_num , pword)\n",
    "pword_sort = sorted(pword, key = lambda d: d[1][6], reverse = True)\n",
    "pword_dside.extend(map(lambda x: x[0], filter(lambda x: x[1][6]>=thresh_rate, pword_sort)))\n",
    "merge = train[['q1', 'q2']]\n",
    "pword_dside_tags = []\n",
    "for i in merge.index:\n",
    "    tags = []\n",
    "    q1_words = set(merge.loc[i, 'q1'].lower().split())\n",
    "    q2_words = set(merge.loc[i, 'q2'].lower().split())\n",
    "    for word in pword_dside:\n",
    "        if (word in q1_words) and (word in q2_words):\n",
    "            tags.append(1.0)\n",
    "        else:\n",
    "            tags.append(0.0)\n",
    "    pword_dside_tags.append(tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 预测具有单侧影响力的字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh_num, thresh_rate = 7, 0.3\n",
    "pword_oside = []\n",
    "pword = sorted_words_power\n",
    "pword = filter(lambda x: x[1][0] * x[1][5] >=thresh_num , pword)\n",
    "pword_oside.extend(map(lambda x: x[0], filter(lambda x: x[1][4]>thresh_rate, pword)))\n",
    "merge = train[['q1', 'q2']]\n",
    "pword_oside_tags = []\n",
    "for i in merge.index:\n",
    "    tags = []\n",
    "    q1_words = set(merge.loc[i, 'q1'].lower().split())\n",
    "    q2_words = set(merge.loc[i, 'q2'].lower().split())\n",
    "    for word in pword_oside:\n",
    "        if (word in q1_words) and (word not in q2_words):\n",
    "            tags.append(1.0)\n",
    "        else:\n",
    "            tags.append(0.0)\n",
    "    pword_oside_tags.append(tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 计算字的双侧影响力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_least = 300\n",
    "merge = train[['q1', 'q2']]\n",
    "words_power = dict(sorted_words_power)\n",
    "pword_dside_rate = []\n",
    "for i in merge.index:\n",
    "    rate = 1.0\n",
    "    q1_words = set(merge.loc[i, 'q1'].lower().split())\n",
    "    q2_words = set(merge.loc[i, 'q2'].lower().split())\n",
    "    share_words = list(q1_words.intersection(q2_words))\n",
    "    for word in share_words:\n",
    "        if word in pword_dside:\n",
    "            rate *= (1.0 - words_power[word][6])\n",
    "    pword_dside_rate.append(1-rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 计算字的单侧影响力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_least = 300\n",
    "merge = train[['q1', 'q2']]\n",
    "words_power = dict(sorted_words_power)\n",
    "pword_oside_rate = []\n",
    "for i in merge.index:\n",
    "    rate = 1.0\n",
    "    q1_words = set(merge.loc[i, 'q1'].lower().split())\n",
    "    q2_words = set(merge.loc[i, 'q2'].lower().split())\n",
    "    q1_diff = list(set(q1_words).difference(set(q2_words)))\n",
    "    q2_diff = list(set(q2_words).difference(set(q1_words)))\n",
    "    all_diff = set(q1_diff + q2_diff)\n",
    "    for word in all_diff:\n",
    "        if word in pword_oside:\n",
    "            rate *= (1.0-words_power[word][4])\n",
    "    pword_oside_rate.append(1-rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 计算可编辑距离"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edit_distance(q1, q2):\n",
    "    str1 = q1.split(' ')\n",
    "    str2 = q2.split(' ')\n",
    "    matrix = [[i+j for j in range(len(str2)+1)] for i in range(len(str1)+1)]\n",
    "    for i in range(1, len(str1)+1):\n",
    "        for j in range(1, len(str2)+1):\n",
    "            if str1[i-1] == str2[j-1]:\n",
    "                d = 0\n",
    "            else:\n",
    "                d = 1\n",
    "            matrix[i][j] = min(matrix[i-1][j]+1, matrix[i][j-1]+1, matrix[i-1][j-1]+d)\n",
    "        if i>1 and j >1 and str1[i-1] == str2[j-2] and str1[i-2] == str2[j-1]:\n",
    "            d = 0\n",
    "            matrix[i][j] = min(matrix[i][j], matrix[i-2][j-2]+d)\n",
    "    return matrix[len(str1)][len(str2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1_len = merge['q1'].apply(lambda x: len(x.split(' '))).values\n",
    "q2_len = merge['q2'].apply(lambda x: len(x.split(' '))).values\n",
    "dist =[edit_distance(merge.loc[i,'q1'],merge.loc[i,'q2'])/np.max([q1_len,q2_len],axis=0)[i] for i in merge.index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 合并提取的字特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjusted_common_word_ratio = pd.DataFrame(adjusted_common_word_ratio)\n",
    "edit_distance = pd.DataFrame(dist)\n",
    "len_diff = pd.DataFrame(len_diff)\n",
    "pword_dside_rate = pd.DataFrame(pword_dside_rate)\n",
    "pword_oside_rate = pd.DataFrame(pword_oside_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features = pd.merge(adjusted_common_word_ratio, edit_distance, left_index = True,right_index =  True)\n",
    "test_features = pd.merge(test_features, len_diff, left_index = True,right_index =  True)\n",
    "test_features = pd.merge(test_features, pword_dside_rate, left_index = True,right_index =  True)\n",
    "test_features = pd.merge(test_features, pword_oside_rate, left_index = True,right_index =  True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features.columns = ['adjusted_common_word_ratio','edit_distance','len_diff','pword_dside_rate','pword_oside_rate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features.to_csv('output/test_feature_words.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 提取test集合字特征"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.DataFrame(test_master)\n",
    "test = pd.merge(test, question, left_on = ['q1'], right_on = ['qid'], how = 'left')\n",
    "test = pd.merge(test, question, left_on = ['q2'], right_on = ['qid'], how = 'left')\n",
    "test = test[['chars_x','chars_y']]\n",
    "test.columns = ['q1', 'q2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_q1 = test.q1.values\n",
    "test_q2 = test.q2.values\n",
    "test_shape_q1 = test_q1.shape[0]\n",
    "test_shape_q2 = test_q2.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'char_to_index' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-01c4f97e0129>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mj\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mtest_q1_indices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchar_to_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'char_to_index' is not defined"
     ]
    }
   ],
   "source": [
    "test_q1_indices = np.zeros((test_shape_q1,max_len))\n",
    "for i in range(test_shape_q1):\n",
    "    sentence_chars_q1 = test_q1[i].split(' ')\n",
    "    for j,w in enumerate(sentence_chars_q1):\n",
    "        if j >= max_len:\n",
    "            break\n",
    "        test_q1_indices[i, j] = char_to_index[w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_q2_indices = np.zeros((test_shape_q2,max_len))\n",
    "for i in range(test_shape_q1):\n",
    "    sentence_chars_q2 = test_q2[i].split(' ')\n",
    "    for j,w in enumerate(sentence_chars_q1):\n",
    "        if j >= max_len:\n",
    "            break\n",
    "        test_q2_indices[i, j] = char_to_index[w]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 计算q1，q2长度差异"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge = test[['q1', 'q2']]\n",
    "q1_len = merge.q1.apply(lambda x: len(x.split(' '))).values\n",
    "q2_len = merge.q2.apply(lambda x: len(x.split(' '))).values\n",
    "len_diff = np.abs((q1_len - q2_len))/np.max([q1_len, q2_len], axis=0)\n",
    "# print(len_diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 计算q1，q2中相同的词的个数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1_word_set = merge.q1.apply(lambda x: x.split(' ')).apply(set).values\n",
    "q2_word_set = merge.q2.apply(lambda x: x.split(' ')).apply(set).values\n",
    "result = [len(q1_word_set[i] & q2_word_set[i]) for i in range(max(len(q1_word_set), len(q2_word_set)))]\n",
    "result = pd.DataFrame(result)\n",
    "result.columns = ['num_common_words']\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 计算共现词比例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio = [len(q1_word_set[i] & q2_word_set[i])/max(q1_len[i], q2_len[i]) for i in range(len(q1_word_set))]\n",
    "ratio = pd.DataFrame(ratio)\n",
    "result.columns = ['common_word_ratio']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 计算tf-idf词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(merge.q2.values)\n",
    "sorted(merge.q2.values)\n",
    "vectorizer = TfidfVectorizer().fit(question.words.values)\n",
    "q1_tfidf = vectorizer.transform(merge.q1.values)\n",
    "q2_tfidf = vectorizer.transform(merge.q2.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 根据tf-idf系数调整共现词比例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjusted_common_word_ratio = []\n",
    "for i in range(0,q1_tfidf.shape[0]):\n",
    "    q1words = {}\n",
    "    q2words = {}\n",
    "    for word in merge.loc[i, 'q1'].split():\n",
    "        q1words[word] = q1words.get(word,0)+1\n",
    "    for word in merge.loc[i, 'q2'].split():\n",
    "        q2words[word] = q2words.get(word,0)+1\n",
    "    sum_shared_word_in_q1 = sum([q1words[w] * q1_tfidf[i, char_to_index[w]] for w in q1words if w in q2words])\n",
    "    sum_shared_word_in_q2 = sum([q2words[w] * q2_tfidf[i,char_to_index[w]] for w in q2words if w in q1words])\n",
    "    sum1 = sum(q1words[w] * q1_tfidf[i,char_to_index[w]] for w in q1words) \n",
    "    sum2 = sum(q2words[w] * q2_tfidf[i,char_to_index[w]] for w in q2words)\n",
    "    sum_tol = sum1 + sum2\n",
    "    if sum_tol<1e-6:\n",
    "        adjusted_common_word_ratio.append(0.0)\n",
    "    else:\n",
    "        adjusted_common_word_ratio.append(1.0 * (sum_shared_word_in_q1 + sum_shared_word_in_q2)/sum_tol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 计算词影响力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_power = {}\n",
    "for i in train.index:\n",
    "    label = int(train.loc[i, 'label'])\n",
    "    q1_words = train.loc[i, 'q1'].lower().split()\n",
    "    q2_words = train.loc[i, 'q2'].lower().split()\n",
    "    all_words = set(q1_words + q2_words)\n",
    "    q1_words = set(q1_words)\n",
    "    q2_words = set(q2_words)\n",
    "    for word in all_words:\n",
    "        if word not in words_power:\n",
    "            words_power[word] = [0. for i in range(7)]\n",
    "        words_power[word][0] += 1.            \n",
    "        words_power[word][1] += 1.\n",
    "        if ((word in q1_words) and (word not in q2_words)) or ((word not in q1_words) and (word in q2_words)):\n",
    "            words_power[word][3] += 1.\n",
    "            if label == 0:\n",
    "                words_power[word][2] += 1.\n",
    "                words_power[word][4] += 1.\n",
    "        if (word in q1_words) and (word in q2_words):\n",
    "            words_power[word][5] += 1.\n",
    "            if label == 1:\n",
    "                words_power[word][2] += 1.\n",
    "                words_power[word][6] += 1.\n",
    "for word in words_power:\n",
    "    words_power[word][1]  /= train.shape[0]\n",
    "    words_power[word][2] /= words_power[word][0]\n",
    "    if words_power[word][3] > 1e-6:\n",
    "        words_power[word][4] /= words_power[word][3]        \n",
    "    words_power[word][5] /= words_power[word][0]\n",
    "sorted_words_power = sorted(words_power.items(), key =lambda d: d[1][0], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 预测具有双侧影响力的词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh_num, thresh_rate = 7, 0.3\n",
    "pword_dside = []\n",
    "pword = sorted_words_power\n",
    "pword = filter(lambda x: x[1][0] * x[1][5] >=thresh_num , pword)\n",
    "pword_sort = sorted(pword, key = lambda d: d[1][6], reverse = True)\n",
    "pword_dside.extend(map(lambda x: x[0], filter(lambda x: x[1][6]>=thresh_rate, pword_sort)))\n",
    "merge = train[['q1', 'q2']]\n",
    "pword_dside_tags = []\n",
    "for i in merge.index:\n",
    "    tags = []\n",
    "    q1_words = set(merge.loc[i, 'q1'].lower().split())\n",
    "    q2_words = set(merge.loc[i, 'q2'].lower().split())\n",
    "    for word in pword_dside:\n",
    "        if (word in q1_words) and (word in q2_words):\n",
    "            tags.append(1.0)\n",
    "        else:\n",
    "            tags.append(0.0)\n",
    "    pword_dside_tags.append(tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 预测具有单侧影响力的词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh_num, thresh_rate = 7, 0.3\n",
    "pword_oside = []\n",
    "pword = sorted_words_power\n",
    "pword = filter(lambda x: x[1][0] * x[1][5] >=thresh_num , pword)\n",
    "pword_oside.extend(map(lambda x: x[0], filter(lambda x: x[1][4]>thresh_rate, pword)))\n",
    "merge = train[['q1', 'q2']]\n",
    "pword_oside_tags = []\n",
    "for i in merge.index:\n",
    "    tags = []\n",
    "    q1_words = set(merge.loc[i, 'q1'].lower().split())\n",
    "    q2_words = set(merge.loc[i, 'q2'].lower().split())\n",
    "    for word in pword_oside:\n",
    "        if (word in q1_words) and (word not in q2_words):\n",
    "            tags.append(1.0)\n",
    "        else:\n",
    "            tags.append(0.0)\n",
    "    pword_oside_tags.append(tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 计算词的双侧影响力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_least = 300\n",
    "merge = train[['q1', 'q2']]\n",
    "words_power = dict(sorted_words_power)\n",
    "pword_dside_rate = []\n",
    "for i in merge.index:\n",
    "    rate = 1.0\n",
    "    q1_words = set(merge.loc[i, 'q1'].lower().split())\n",
    "    q2_words = set(merge.loc[i, 'q2'].lower().split())\n",
    "    share_words = list(q1_words.intersection(q2_words))\n",
    "    for word in share_words:\n",
    "        if word in pword_dside:\n",
    "            rate *= (1.0 - words_power[word][6])\n",
    "    pword_dside_rate.append(1-rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 计算词的单侧影响力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_least = 300\n",
    "merge = train[['q1', 'q2']]\n",
    "words_power = dict(sorted_words_power)\n",
    "pword_oside_rate = []\n",
    "for i in merge.index:\n",
    "    rate = 1.0\n",
    "    q1_words = set(merge.loc[i, 'q1'].lower().split())\n",
    "    q2_words = set(merge.loc[i, 'q2'].lower().split())\n",
    "    q1_diff = list(set(q1_words).difference(set(q2_words)))\n",
    "    q2_diff = list(set(q2_words).difference(set(q1_words)))\n",
    "    all_diff = set(q1_diff + q2_diff)\n",
    "    for word in all_diff:\n",
    "        if word in pword_oside:\n",
    "            rate *= (1.0-words_power[word][4])\n",
    "    pword_oside_rate.append(1-rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 合并提取的词特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjusted_common_char_ratio = pd.DataFrame(adjusted_common_word_ratio)\n",
    "pchar_dside_rate = pd.DataFrame(pword_dside_rate)\n",
    "pchar_oside_rate = pd.DataFrame(pword_oside_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features = pd.read_csv('output/test_feature_words.csv', index_col=0)\n",
    "test_features = pd.DataFrame(test_features)\n",
    "test_features = pd.merge(test_features, adjusted_common_char_ratio, left_index = True,right_index =  True)\n",
    "test_features = pd.merge(test_features, pchar_dside_rate, left_index = True,right_index =  True)\n",
    "test_features = pd.merge(test_features, pchar_oside_rate, left_index = True,right_index =  True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features.columns = ['adjusted_common_word_ratio','edit_distance','len_diff',\n",
    "                          'pword_dside_rate','pword_oside_rate','adjusted_common_char_ratio','pchar_dside_rate','pchar_oside_rate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features.to_csv('output/test_feature.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(172956, 8)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   # 模型建立"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.DataFrame(train_master)\n",
    "train = pd.merge(train, question, left_on = ['q1'], right_on = ['qid'], how = 'left')\n",
    "train = pd.merge(train, question, left_on = ['q2'], right_on = ['qid'], how = 'left')\n",
    "train = train[['label', 'words_x','words_y']]\n",
    "train.columns = ['label', 'q1', 'q2']\n",
    "train_q1 = train.q1.values\n",
    "train_q2 = train.q2.values\n",
    "train_label = train.label.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.DataFrame(test_master)\n",
    "test = pd.merge(test, question, left_on = ['q1'], right_on = ['qid'], how = 'left')\n",
    "test = pd.merge(test, question, left_on = ['q2'], right_on = ['qid'], how = 'left')\n",
    "test = test[['words_x','words_y']]\n",
    "test.columns = ['q1', 'q2']\n",
    "test_q1 = test.q1.values\n",
    "test_q2 = test.q2.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = pd.read_csv('output/train_feature.csv', index_col=0)\n",
    "test_features = pd.read_csv('output/test_feature.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object _BaseKFold.split at 0x1a112a4150>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kf.split(train_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def trainLSTM(train_q1, train_q2, train_label, embed_matrix, test_q1, test_q2, train_features, test_features, kfold):\n",
    "   \n",
    "    lstm_num = 75\n",
    "    lstm_drop = 0.5\n",
    "    BATCH_SIZE = 256  # 128\n",
    "    \n",
    "    for idx_train, idx_val in kf.split(train_features):\n",
    "\n",
    "        q1_train = train_q1[idx_train]\n",
    "        q2_train = train_q2[idx_train]\n",
    "        y_train = train_label[idx_train]\n",
    "        f_train = train_features[idx_train]\n",
    "       \n",
    "        q1_val = train_q1[idx_val]\n",
    "        q2_val = train_q2[idx_val]\n",
    "        y_val = train_label[idx_val]\n",
    "        f_val = train_features[idx_val]\n",
    "    \n",
    "        # Define the model\n",
    "        question1 = Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
    "        question2 = Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
    "\n",
    "        embed_layer = Embedding(embed_matrix.shape[0], EMBEDDING_DIM, weights=[embed_matrix],\n",
    "                                input_length=MAX_SEQUENCE_LENGTH, trainable=False)\n",
    "\n",
    "        q1_embed = embed_layer(question1)\n",
    "        q2_embed = embed_layer(question2)\n",
    "\n",
    "        shared_lstm_1 = LSTM(lstm_num, return_sequences=True)\n",
    "        shared_lstm_2 = LSTM(lstm_num)\n",
    "\n",
    "        q1 = shared_lstm_1(q1_embed)\n",
    "        q1 = Dropout(lstm_drop)(q1)\n",
    "        q1 = BatchNormalization()(q1)\n",
    "        q1 = shared_lstm_2(q1)\n",
    "        # q1 = Dropout(0.5)(q1)\n",
    "\n",
    "        q2 = shared_lstm_1(q2_embed)\n",
    "        q2 = Dropout(lstm_drop)(q2)\n",
    "        q2 = BatchNormalization()(q2)\n",
    "        q2 = shared_lstm_2(q2)\n",
    "        # q2 = Dropout(0.5)(q2)   # of shape (batch_size, 128)\n",
    "\n",
    "        # 求distance (batch_size,1)\n",
    "        d = Subtract()([q1, q2])\n",
    "        #distance = Dot(axes=1, normalize=False)([d, d])\n",
    "        #distance = Lambda(lambda x: K.abs(x))(d)\n",
    "        distance = Multiply()([d, d])\n",
    "        # 求angle (batch_size,1)\n",
    "        # angle = Dot(axes=1, normalize=False)([q1, q2])\n",
    "        angle = Multiply()([q1, q2])\n",
    "        # merged = concatenate([distance,angle])\n",
    "\n",
    "        # magic featurues\n",
    "        magic_input = Input(shape=(train_features.shape[1],))\n",
    "        magic_dense = BatchNormalization()(magic_input)\n",
    "        magic_dense = Dense(64, activation='relu')(magic_dense)\n",
    "        #magic_dense = Dropout(0.3)(magic_dense)\n",
    "        \n",
    "        merged = concatenate([distance,angle,magic_dense])\n",
    "        merged = Dropout(0.3)(merged)\n",
    "        merged = BatchNormalization()(merged)\n",
    "\n",
    "        merged = Dense(256, activation='relu')(merged)  # 64\n",
    "        merged = Dropout(0.3)(merged)\n",
    "        merged = BatchNormalization()(merged)\n",
    "\n",
    "        merged = Dense(64, activation='relu')(merged)  # 64\n",
    "        merged = Dropout(0.3)(merged)\n",
    "        merged = BatchNormalization()(merged)\n",
    "\n",
    "        is_duplicate = Dense(1, activation='sigmoid')(merged)\n",
    "\n",
    "        model = Model(inputs=[question1, question2, magic_input], outputs=is_duplicate)\n",
    "\n",
    "        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "        model.summary()\n",
    "        \n",
    "        # define save model \n",
    "        best_weights_filepath = 'models/0715 lstm keras/word_lstm_with_magics_' + str(model_count) + '.hdf5'\n",
    "        earlyStopping = kcallbacks.EarlyStopping(monitor='val_loss', patience=3, verbose=1, mode='auto')\n",
    "        saveBestModel = kcallbacks.ModelCheckpoint(best_weights_filepath, monitor='val_loss', verbose=1,\\\n",
    "                                                   save_best_only=True, mode='auto')\n",
    "\n",
    "        hist = model.fit([q1_train, q2_train, f_train], \n",
    "                         y_train,\n",
    "                         validation_data=([q1_val, q2_val, f_val], y_val),\n",
    "                         epochs=30, \n",
    "                         batch_size=BATCH_SIZE, \n",
    "                         shuffle=True,\n",
    "                         callbacks=[earlyStopping, saveBestModel], \n",
    "                         verbose=1)\n",
    "\n",
    "        model.load_weights(best_weights_filepath)\n",
    "        print(model_count, \"validation loss:\", min(hist.history[\"val_loss\"]))\n",
    "        best_vali_score[model_count] = min(hist.history[\"val_loss\"])\n",
    "        \n",
    "        # predict on the val set\n",
    "        preds = model.predict([q1_val, q2_val, f_val], batch_size=1024, verbose=1)\n",
    "        val_preds = pd.DataFrame({\"y_pre\": preds.ravel()})\n",
    "        val_preds['val_index'] = idx_val\n",
    "        save_path = 'features/0715_lstm_word_with_magic/vali_' + str(model_count) + '.csv'\n",
    "        val_preds.to_csv(save_path, index=0)\n",
    "        print(model_count, \"val preds saved.\")\n",
    "        \n",
    "        # predict on the test set\n",
    "        preds1 = model.predict([test_q1, test_q2, test_features], batch_size=1024, verbose=1)\n",
    "        test_preds = pd.DataFrame({\"y_pre\": preds1.ravel()})\n",
    "        save_path1 = 'features/0715_lstm_word_with_magic/test_' + str(model_count) + '.csv'\n",
    "        test_preds.to_csv(save_path1, index=0)\n",
    "        print(model_count, \"test preds saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'[ 25439  25440  25441 ... 254383 254384 254385] not in index'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-0e51051fd0e1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mkfold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKFold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_splits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrainLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_q1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_q2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membed_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_q1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_q2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkfold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-15-2e402c822686>\u001b[0m in \u001b[0;36mtrainLSTM\u001b[0;34m(train_q1, train_q2, train_label, embed_matrix, test_q1, test_q2, train_features, test_features, kfold)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mq2_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_q2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx_train\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_label\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx_train\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mf_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_features\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx_train\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mq1_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_q1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx_val\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2677\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIndex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2678\u001b[0m             \u001b[0;31m# either boolean or fancy integer index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2679\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2680\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2681\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_getitem_array\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2721\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_take\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2722\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2723\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_to_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2724\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_take\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2725\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_convert_to_indexer\u001b[0;34m(self, obj, axis, is_setter)\u001b[0m\n\u001b[1;32m   1325\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m                     raise KeyError('{mask} not in index'\n\u001b[0;32m-> 1327\u001b[0;31m                                    .format(mask=objarr[mask]))\n\u001b[0m\u001b[1;32m   1328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values_from_object\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: '[ 25439  25440  25441 ... 254383 254384 254385] not in index'"
     ]
    }
   ],
   "source": [
    "kfold = KFold(n_splits = 10)\n",
    "\n",
    "trainLSTM(train_q1, train_q2, train_label, embed_matrix, test_q1, test_q2, train_features, test_features, kfold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20891, 301)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_embed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20891, 300)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def load_dataset(max_seq_len, embed_dim, word_level=True):\n",
    "    '''\n",
    "    读取数据，对数据进行预处理，并生成embed_matrix\n",
    "    '''\n",
    "    #1、读取数据，数据预处理\n",
    "    #数据路径\n",
    "    question_path = os.path.join('datasets', 'question.csv')\n",
    "    train_path = os.path.join('datasets', 'train.csv')\n",
    "    if word_level:\n",
    "        embed_path = os.path.join('datasets', 'word_embed.txt')\n",
    "    else:\n",
    "        embed_path = os.path.join('datasets', 'char_embed.txt')\n",
    "    \n",
    "    #读取数据\n",
    "    question = pd.read_csv(question_path)\n",
    "    train = pd.read_csv(train_path)\n",
    "\n",
    "    # 把train里面的问题id匹配到句子\n",
    "    train = pd.merge(train,question,left_on=['q1'],right_on=['qid'],how='left')\n",
    "    train = pd.merge(train,question,left_on=['q2'],right_on=['qid'],how='left')\n",
    "    \n",
    "    if word_level:\n",
    "        train = train[['label','words_x','words_y']]\n",
    "    else:\n",
    "        train = train[['label','chars_x','chars_y']]\n",
    "    train.columns = ['label','q1','q2']\n",
    "    \n",
    "    # 读取word_to_vec_map，注意这里的index是word id\n",
    "    word_to_vec_map = pd.read_csv(embed_path, sep=' ', header=None, index_col=0)\n",
    "    \n",
    "    # 先定义两个字典，实现wid与(positive) index的相互转换,注意index从1开始\n",
    "    word = word_to_vec_map.index.values\n",
    "    word_to_index = dict([(word[i],i+1) for i in range(len(word))])\n",
    "    index_to_word = dict([(i+1, word[i]) for i in range(len(word))])\n",
    "    \n",
    "    # 把句子转换成int indices,并zero pad the sentance to max_seq_len\n",
    "    train_q1_indices = sentences_to_indices(train.q1.values, word_to_index, max_seq_len)\n",
    "    train_q2_indices = sentences_to_indices(train.q2.values, word_to_index, max_seq_len)\n",
    "    label = train.label.values\n",
    "    \n",
    "    #3、生成embeding_matrix, index为整数，其中index=0,对应的是np.zeros(300),0向量，对应我们padding的值\n",
    "    vocab_len = len(word_to_index) + 1                                   \n",
    "    # Initialize the embedding matrix as numpy arrays of zeros\n",
    "    embed_matrix = np.zeros((vocab_len, embed_dim))\n",
    "    # Set each row \"index\" of the embedding matrix to be the word vector representation of the \"index\"th word of the vocabulary\n",
    "    for word, index in word_to_index.items():\n",
    "        embed_matrix[index, :] = word_to_vec_map.loc[word].values\n",
    "\n",
    "    return (train_q1_indices,train_q2_indices, label, embed_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_q1, train_q2, train_label, embed_matrix = load_dataset(20, 300, word_level=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentences_to_indices(X, word_to_index, max_len):\n",
    "    \"\"\"\n",
    "    Converts an array of sentences (strings) into an array of indices corresponding to words in the sentences.\n",
    "    The output shape should be such that it can be given to `Embedding()` \n",
    "    \n",
    "    Arguments:\n",
    "    X -- array of sentences (strings), of shape (m, 1)\n",
    "    word_to_index -- a dictionary containing the each word mapped to its index\n",
    "    max_len -- maximum number of words in a sentence. You can assume every sentence in X is no longer than this. \n",
    "    \n",
    "    Returns:\n",
    "    X_indices -- array of indices corresponding to words in the sentences from X, of shape (m, max_len)\n",
    "    \"\"\"\n",
    "    m = X.shape[0]            # number of training examples \n",
    "    # Initialize X_indices as a numpy matrix of zeros and the correct shape (≈ 1 line)\n",
    "    X_indices = np.zeros((m, max_len))\n",
    "    for i in range(m):                               # loop over training examples\n",
    "        # split the sentences into words\n",
    "        sentence_words =X[i].split(' ') \n",
    "        # Loop over the words of sentence_words\n",
    "        for j,w in enumerate(sentence_words):\n",
    "            if j >= max_len:\n",
    "                break\n",
    "            # Set the (i,j)th entry of X_indices to the index of the correct word.\n",
    "            X_indices[i, j] = word_to_index[w]\n",
    "            \n",
    "    return X_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
